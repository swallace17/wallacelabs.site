[{"content":"I run my home network on Ubiquiti UniFi based hardware utilizing a UniFi Dream Machine Pro (UDMP) as my gateway/firewall, along with an assortment of UniFi Access Points (APs) and managed switches. In the event that I need to remote into my network, my gateway operates an L2TP over IPsec VPN. Using the built in Radius server, I\u0026rsquo;ve been able to configure this VPN so that remote clients are automatically routed to a specific subnet via a VLAN tag. This has been great, because I can give myself access to my full network, while limiting other clients to isolated sections of my network as desired.\nRecently, however, I began testing Ubiquiti\u0026rsquo;s UID (UniFi Identity) product as an Early Access member. I won\u0026rsquo;t spend too much time on this, as the product is very much still in development (and I believe the Early Access terms of service prevent me from doing so anyway), but suffice it to say that utilizing UID has put a temporary crimp in my normal VPN operations. In short, the UID platform, when integrated on my firewall, runs all VPN authentication through its identity management platform (which can integrate with LDAP, Active Directory, etc). This functionality is very cool, but not fully fleshed out yet. My problem is that, for the time being, there is no way to assign a given user a vlan tag. This prevents me from locking certain users to particular subnets where firewall rules prevent them from accessing the network at large.\nUbiquiti has confirmed that this functionality will be coming eventually, but in the meantime I\u0026rsquo;m stuck, as tagging VPN users to a vlan is something I regularly need to do. It is with all this in mind that I began considering operating a second VPN server directly from an isolated subnet as a temporary work-around.\nImplementation Considerations With an action plan worked out, I fiddled around with a few different approaches. I briefly considered setting up an OpenVPN server, but quickly moved on to Wireguard due to its security, speed, and relative simplicity.\nOperating the server as a Docker container was also a big deal, as I have containerized practically all of my Homelab\u0026rsquo;s infrastructure at this point. Generally speaking, if I can\u0026rsquo;t run it as a container, I\u0026rsquo;ll find another solution. Dedicated application environments are too important, and accomplishing this isolation with Virtual Machines is too resource-expensive for it to be an ongoing solution. Containers or bust!\nLastly, I wanted to run all this using my Unraid Server as a host. For the unfamiliar, Unraid is a Linux based OS focused on enabling Network Attached Storage (NAS) functions and VM/Container hosting. My Unraid server is where I host a good deal of containerized infrastructure for my home\u0026ndash; no reason to run this Wireguard server anywhere else.\n Note: The Unraid OS actually has a built-in Wireguard server. Why not just use that? In theory, I could, and I did try. The reason I am not is because my goal in all of this is to have clients connected to this VPN restricted to a given subnet. Unfortunately, any clients connected to Unraid\u0026rsquo;s built-in Wireguard server end up on the same subnet as the Unraid server itself. In my case, my Unraid server is connected to an untagged network port, and thus is on my primary LAN network, not on a restricted subnet where I want these clients to end up. As of the time of writing, there is no way to assign the built-in Wireguard service to a virtual, vlan-tagged network interface. The good news is that Unraid OS does support assigning vlan-tagged virtual network interfaces to any VM or container it hosts\u0026ndash; thus my goal of operating a Wireguard server as a container.\n Making It Happen The first step in getting this server online was to find a Docker Image designed for creating a containerized Wireguard server.\n I initially thought I\u0026rsquo;d build one by hand, and got most of the way through the process before deciding it wasn\u0026rsquo;t worth the effort for what will ultimately be a temporary solution anyway. That being said, I learned a lot, and if you\u0026rsquo;d like to set down that path yourself, I fully recommend following this excellent guide written by Jamon Camisso at Digital Ocean.\n After a bit of research, there were a couple images I was considering. The first was docker-wireguard by the great LinuxServer.io group. Second, and what I\u0026rsquo;ve ended up going with, is wg-easy. The linuxserver.io wireguard container is more flexible, but wg-easy ended up being easier for my purposes, and offers the great benefit of hosting a cool web-management panel for creating client connections, viewing their QR codes, downloading their configuration files, etc. All things that can be trivially done via CLI, but the GUI web panel makes things a bit easier, and prettier for every-day use.\nGreat, I found an image on DockerHub that does exactly what I need it to. All thats left is to get it running on my Unraid server, right? Unfortunately, thats not where this story ends.\nThe Problem Recall that the whole point of this endeavor was to give VPN clients access to a subnet of my network only, not the whole thing. Furthermore, recall that any clients connected to the Wireguard server will have access to the same network the Wireguard server itself resides on. With that in mind, the only way to restrict Wireguard clients to a given subnet is to operate the Wireguard server on that same subnet.\n Note: This limitation could be avoided if the wireguard server were being run on the gateway/router, but UniFi does not currently support running a Wireguard server, and again there is the complication introduced by UID. All of this is a long way of saying if you have a PFSense gateway (or another gateway that supports operating a wireguard server), you could probably work around this issue. Beyond that, you could probably get it working with a multi-gateway type setup, with a dedicated PFsense box that handles some routing, but is not your primary \u0026ldquo;router\u0026rdquo;, per se\u0026hellip; but that is entering into a world of networking complexity that goes far beyond the scope of this post. A post for another day, perhaps.\n So, I\u0026rsquo;ve got a Wireguard server docker container. How do I go about getting it on the correct subnet? The best way to get this container on my subnet of choice is to run the container using host networking mode, instead of bridge. On the host, I created a virtual network interface, tagged it to the relevant vlan, and assigned that interface to my container running in host networking mode. Simple enough, right? Well, it should be. Creating the interface is no big deal on Unraid. Just have to open up the Unraid web-management panel, navigate to Settings--\u0026gt;Network Settings, enable VLANS, and configure a virtual interface to be tagged to a vlan, as seen below:\n The same thing should also be simple enough to implement on Windows, on macOS, or generically on Linux. That being said, I have not tested it. This walk-through is assuming the use of Unraid, I just wanted to make it clear that there\u0026rsquo;s no reason this could not be done on another platform.\nHere is the problem- the wg-easy container does not run correctly when configured to run in host-networking mode. The container starts up correctly, the Wireguard server runs, and clients can even connect- but they cannot seem to pass any data. I\u0026rsquo;ve been unable to determine a solution, and not for lack of trying. There is an open issue on the container\u0026rsquo;s Github page, and I\u0026rsquo;ve asked around on the container\u0026rsquo;s support page on the Unraid Forums. So far, nothing has proved useful.\nI have not made much headway, but I\u0026rsquo;m still hoping to find a solution. Though I am reluctant to give up on the convince of the web-management panel, I\u0026rsquo;ve been considering trying the LinuxServer.io Wireguard container (with host networking) and seeing if that works. If it does, it might prove helpful in determining what is wrong in wg-easy when running host networking. I\u0026rsquo;ve also considered revisiting building a wireguard server container from scratch, hoping it might give me a better \u0026ldquo;under the hood\u0026rdquo; understanding of wireguard such that I can better determine what is breaking in wg-easy when running in host networking.\nUltimately, unless someone else posts a solution on Github, solving this issue is going to involve some degree of reverse engineering the wg-easy container, and and I go back and forth on whether its worth the time investment. This was supposed to be a quick, temporary solution! It has turned into a whole project, like these things often do\nEating My Words In the meantime, to solve my problem day to day, I have been eating my words about only running infrastructure inside containers. I setup a Linux VM on my Unraid server, and installed Docker on that VM for some nested virtualization action. Still using containers! Just nested inside a VM, unfortunately. This solves my problem because I am able to assign the VLAN tagged virtual network interface to the VM. Then, when the container runs with bridge networking inside the VM, any connected Wireguard clients end up on the correct subnet, just as I\u0026rsquo;ve been hoping to achieve this whole time. Its all a bit more convoluted than I would like, but it works nonetheless.\nIf you would like to implement a similar solution, it should be fairly simple. First, log on to Unraid and navigate to Settings--\u0026gt;Network Settings, enable VLANS, and configure a virtual interface to be tagged to a vlan (as discussed earlier).\n Quick Aside: Be aware that you\u0026rsquo;ll need to make sure the network port your Unraid server is connected to is configured for both your untagged native network, and tagged for your desired vlan. It might even make sense to set it up as a trunk port (carries all vlans) if you intend to use all vlans on your Unraid server for different VM\u0026rsquo;s, containers, etc. Alternately, if your server has multiple network ports (via a dedicated PCIe NIC, or the like), you could just connect your server to multiple ports and use dedicated physical interfaces. Lots of options here.\n Then, configure a VM of your choice. I initially setup a Windows 11 VM for this, being curious how WSL and Docker Desktop would behave utilizing nested virtualization. Turns out, the answer is not well. The VM maxes out all assigned CPU cores and freezes up after a few minutes idle. Did some cursory troubleshooting, and there appears to be no solution at the moment. With that in mind, I\u0026rsquo;d recommend a Linux OS. Ubuntu or Debian are usually my go-to for a quick VM, but its really just preference. Just make sure to assign your vlan tagged virtual network interface to that VM.\nWith your VM running, install Docker, and follow the instructions on the wg-easy Github page to get your Wireguard server running. When you\u0026rsquo;re done, you\u0026rsquo;ll have a wireguard server running such that all connecting clients are restricted to your desired subnet. Mission accomplished!\nI hope you might find my documented struggles here useful, or entertaining at the very least. If you happen to have a solution to running wg-easy in host networking mode, please let me know! I\u0026rsquo;d be thrilled. Just hit me up on the Github issue I linked earlier, or on the site\u0026rsquo;s Discord server. Thanks!\n","permalink":"https://wallacelabs.tech/posts/operating-an-unraid-hosted-containerized-wireguard-vpn-server-on-a-vlan/","summary":"I run my home network on Ubiquiti UniFi based hardware utilizing a UniFi Dream Machine Pro (UDMP) as my gateway/firewall, along with an assortment of UniFi Access Points (APs) and managed switches. In the event that I need to remote into my network, my gateway operates an L2TP over IPsec VPN. Using the built in Radius server, I\u0026rsquo;ve been able to configure this VPN so that remote clients are automatically routed to a specific subnet via a VLAN tag.","title":"Operating an Unraid-Hosted, Containerized Wireguard VPN Server on a VLAN"},{"content":"Google Workspace Enterprise plans (successor to G Suite Business plans) include unlimited Google Drive storage for licensed users, but there is a strict 750Gb limit on data a licensed user can upload to their account within 24 hours. This limit is generally quite reasonable and should not pose an issue. The vast majority of people today do not have internet connections sufficient to upload 750Gb of data within 24 hours even if they wanted to. That being said, some people do have sufficient connections, and I\u0026rsquo;m happy to count myself among them (Thanks, altafiber!).\nWith a 250Mb/s upload speed, I have personally run into situations where I happen to need a lot of data, say 800Gb (just over the daily limit), uploaded ASAP. With my connection, I should be able to upload that data in a little more more than 7.5 hours, but instead I\u0026rsquo;m stuck waiting at least 24. Not the end of the world, but frustrating nonetheless. Recently, I used the time I was stuck waiting due to this limit to explore if there were any ways around it. Turns out, there\u0026rsquo;s a couple!\nThe Quick Way The easiest solution is to compress all the data you want to upload to a single file. Zip, Tar, Rar, whatever, just create a compressed archive of your data. There\u0026rsquo;s a million tools out there to make that happen (including those built-in to your OS), but I recommend 7-Zip on Windows, and Keka on macOS. Once you have created your archive, Google Drive will allow you to upload a single file (up to 5Tb in size) without cutting you off after you\u0026rsquo;ve uploaded 750Gb. As soon as that upload completes, assuming 750Gb worth of that file were uploaded within the last 24 hours, you will have triggered your limit and be unable to upload anything else for ~24 hours. Its difficult to tell exactly when the system will unlock and allow uploads again, but hey, at least you got your data uploaded ASAP.\nThe Interesting Way What if, for whatever reason, you don\u0026rsquo;t want to compress your data to an archive? Maybe you don\u0026rsquo;t have enough disk space to store your data and a zip of your data at the same time. There is another way! It involves the scripted use of rclone and Google Workspace service accounts. I do not necessarily recommend this work-around, and highly doubt Google would smile upon it, but it does exist, is technically interesting, and I\u0026rsquo;m hardly the first to write about it\u0026ndash; so lets discuss.\nThe general premise of this work-around is embracing the fact that a single account cannot upload more than 750Gb in a day. No way around that fact. So if a single account can\u0026rsquo;t do it, just use multiple! Simple enough in theory, but takes a bit of setup to implement. Also, while this method does theoretically enable abuse of the service, I do not think utilizing it is inherently so. In my opinion, this method can be utilized ethically so long as it is utilized sparingly, and for data which does not exceed 5Tb in size. Google will, after all, accept 5Tb within a day anyway, so long as you compress first. This just saves you a bit of trouble. I personally have all this setup on my linux-based NAS, in the event I ever need to upload more than 750Gb of data on short notice.\n Note I am hardly the first to discover or publicize this method. There are projects floating around online which enable automating this work-around to great effect, allowing uploading of up to 75Tb a day (100x the usual limit). While I find these projects interesting academically, they are more complex to implement than the method I\u0026rsquo;ve detailed below. Furthermore, their entire architecture seems to be built around enabling mass abuse of the Google Workspace Enterprise storage system. Unless you plan to abuse that system en masse (and dare Google to terminate your account while you\u0026rsquo;re at it), I don\u0026rsquo;t think there is much practical need for the additional complexity these projects introduce.\n Implementation Implementing this method for yourself is relatively simple. Quick, high level overview: We\u0026rsquo;re going to setup some Google Drive service accounts, create a Shared Drive between you and the service accounts, and setup a bash script to automatically utilize and cycle through those service accounts by uploading to Google Drive via rclone. What is rclone? rclone is a great CLI-based tool which enables easily moving data around from one place to another, much like rsync, except it supports cloud services! It\u0026rsquo;s the only Google drive client I\u0026rsquo;m aware of that will allow us to programmatically cycle through different accounts as we upload data.\nCreating Service Accounts To make this happen, we\u0026rsquo;ll begin by creating a couple service accounts. Why service accounts? Service accounts are able to upload 750Gb of data to Google Drive a day, just like a normal account, but they are not a full user account, so you will not be charged licensing fees for them like you would a normal user.\nTo create a service account, go to the Google Developer Console. You must have a project\u0026ndash; if you do not already have one, create one. Open your project, click on the hamburger menu in the top left, and navigate to IAM \u0026amp; Admin--\u0026gt; Service Accounts. Click the Create Service Account button. You will see the below Create Service Account screen, with 3 steps.\n Set a service account name and description. Click Create and Continue. Skip through optional steps 2 and 3.\n  Click the 3-dot actions button on your newly created service account, and navigate to the \u0026ldquo;Manage Keys\u0026rdquo; tab. From there, click Add Key--\u0026gt;Create New Key--\u0026gt;JSON--\u0026gt;Create. You\u0026rsquo;ll be prompted to download the JSON key you just created. Do so, and save it somewhere memorable.\nRepeat these steps to create at least one more service account. In theory, you could create more. Each account can upload 750Gb of data before it hits its limit and we move on to the next one. Personally I setup 4. With my upload speed of 250Mb/s, each account will upload for ~7.5 hours before the 750Gb limit is hit. This allows me to cycle through the 4 accounts over about a 30 hour period before restarting back at beginning and continuing uploading. This worked out great when I needed a little over 3Tb of data uploaded ASAP to share with a co-worker, and zipping the data to an archive was not practical. Do your own math to figure out how many accounts you\u0026rsquo;ll need to keep your upload live for a full 24-hour period.\n Note: When I earlier mentioned projects floating around on the internet\u0026ndash; The process of creating projects, groups, and service accounts is the main thing these tools automate. In my opinion, a responsible use of this workaround will use, at most, a handful of service accounts. These tools will create multiple projects, all with the maximum 100 service accounts, enabling uploading truly absurd amounts of data as fast as one\u0026rsquo;s upload connection will carry it. A single set of 100 service accounts is \u0026ldquo;limited\u0026rdquo; at uploading 75Tb a day! Its because of this that I believe there is no reason to use these tools unless you intend to egregiously abuse the system. I recommend just setting up a handful of service accounts by hand- as I\u0026rsquo;m outlining here in this post.\n With service accounts created, you should see something like this.\n Adding Service Accounts to a Group Time to move on to creating a group, where each of the service accounts you just created will be a member. This is helpful because when we create a shared drive, you don\u0026rsquo;t need to add every service account to it individually, you can just add the group and be done. That being said, we are not creating many service accounts, so you could skip this step and just add the service accounts to the shared drive directly, if you want. Might even be faster if you just have 2 accounts.\nBegin by logging into your Google Workspace admin panel at admin.google.com. From the admin panel, navigate to Directory\u0026ndash;\u0026gt;Groups on the left-hand sidebar, then click \u0026ldquo;Create Group\u0026rdquo;.\n On the Create Group screen, add a name for the group, an email for the group, and set your account to be the group owner.\n Click next, and on Group Settings, configure the following:\n Begin by clicking the Restricted preset, then change \u0026lsquo;Who can join the group?\u0026rsquo; to Only Invited Users, and finish by changing the \u0026lsquo;Allow members outside your organization\u0026rsquo; toggle to ON.\n  With the group created, click the Add members, and type in the email address for each of the service accounts you created earlier. When you\u0026rsquo;re done, click add to group, and we\u0026rsquo;re almost done with the Google Workspace side of things. Last up, you need to create a shared drive. This shared drive is where all your service accounts will be uploading data to, shared with your primary account.\nCreating a Shared Drive To create a shared drive, navigate to drive.google.com in your browser. Sign into your account, click Shared Drives in the left-hand navigation bar, and click the New button in the top left. Name your shared drive whatever you like and click create. Your shared drive will be automatically opened, then click the Manage Members button. Now, type in the email you assigned to the group you just created, and grant the group Content Manager permissions. This will allow the service accounts in this group to access the shared drive. Consequently, rclone, running logged into these accounts, will be able to read/write/delete files in this shared drive as necessary.\nInstall rclone and Configure Remotes First, install rclone. It\u0026rsquo;s available for many Operating Systems, including Windows, macOS, and Linux. Once rclone is installed, open up a Powershell/Terminal session, and type the following command:\nrclone config Follow the prompts to create a new remote. Give it a name and set it to Google Drive using option 16. Now you\u0026rsquo;ll need to generate an Application Client ID. This ID is what identifies your instance of rclone to the Google Drive API. If you have any trouble following my next few steps, the rclone-provided instructions may prove helpful, but I think you\u0026rsquo;ll find them quite similar.\nBegin by again logging into the Google Developer Console. Open up the same project you used earlier while creating the service accounts. Using the left-hand navigation bar, navigate to APIs \u0026amp; Services--\u0026gt; Enabled APIs \u0026amp; services. Click the + Enabled APIs and Services button at the top. This will open up the API library. Here, we are going to select the Google Drive API and enable it for this project. Search for Google Drive and enable it, like this:\n Now that you\u0026rsquo;ve enabled the Google Drive API for this project, you can create a credential for rclone to utilize this API by navigating to APIs \u0026amp; Services--\u0026gt; Credentials. Click + Create Credentials and select the OAuth client ID option. You will see the following:\n Select Desktop App as the application type, and give the OAuth 2.0 client a name\n  Click create and you will be presented with a client ID and a client secret. Be sure to record both, ideally in your password manager (I recommend 1Password).\n Lastly, navigate to APIs \u0026amp; Services--\u0026gt; OAuth Consent Screen and click the Publish App button to go live with \u0026ldquo;your app\u0026rdquo; (rclone). Now, with your Client ID created, return to your Powershell/Terminal Session. Copy the Client ID you recorded a moment ago, and paste it into this CLI session. Next, do the same thing with your Client Secret. Continuing on, select option 1 to allow Full access to all files. You will now be asked to specify a root folder. Leaving this blank will allow rclone to download/upload to any folder in your Google Drive. You can also specify a path to lock rclone\u0026rsquo;s access to a specific directory (see rclone\u0026rsquo;s Google Drive documentation for more info).\nNext up, you\u0026rsquo;ll be asked to specify the path to your Service Account JSON credentials. This is what allows rclone to run logged in as a service account. You downloaded them earlier, while setting up your service accounts. Decide where you\u0026rsquo;d like them to \u0026ldquo;live\u0026rdquo; on your system long term, and copy/paste that path into your CLI session. Continue to follow the prompts until you are asked \u0026ldquo;Configure this as a Shared Drive (Team Drive)?\u0026rdquo; Type y, and select the team drive you created earlier. Then wrap up by selecting Yes this is OK and you\u0026rsquo;re good to go!\nAll that is left now is to repeat this process for each of your service accounts. Don\u0026rsquo;t worry, you can use the same client-ID and client-secret for all of them, so the remaining rclone remote configuration should go much more quickly. Just be sure that you specify a new Service Account JSON credential for each one!\nWith all your rclone remotes are configured, we\u0026rsquo;re nearly done. Now you just need to utilize them with a script!\nAuto-Cycle Through Service-Accounts with Scripts! The below bash script will allow you to tie together all the work you\u0026rsquo;ve done thus far. It will use rclone to begin uploading data to your Shared drive, utilizing a service account until 750Gb has been uploaded, and then switching to the next.\n You will need to substitute in your own path for the data you want to upload. Also, you will need to swap in the names of the rclone remotes you created for each of your service accounts. If you have less than 4 remotes, remove lines as necessary, if you have more, add additional ones in the same format.\nAs I mentioned before, with my internet connection, it takes me ~7.5 hours to upload 750Gb. By the time all 4 of my service accounts have cycled through 750Gb uploaded, I no longer have an upload lock on the first account. With that in mind, If I have more than 3,000Gb to upload, I could simply copy and paste these 4 rclone commands to the end of the script, and it would cycle through every service account twice, uploading nearly 6Tb of data when its all said and done\u0026ndash; but then things start to get hairy again, when it comes to abusing the service. Muddy waters, use your best judgment, please don\u0026rsquo;t abuse the service, you risk your account getting terminated, use at your own risk, don\u0026rsquo;t give Google reasons to discontinue unlimited cloud storage plans, blah blah blah. I can\u0026rsquo;t stop you, the information is out there, but its worth bearing in mind nonetheless.\nThis script is provided in bash, for easy use on Linux and macOS systems. You should be able to easily replicate it in Powershell, should you so desire, or just run it via Windows Subsystem for Linux (WSL). In theory, you could even automate the running of this script using cron jobs, or task scheduler on Windows, to regularly upload data without worrying about upload limits slowing you down (again, abuse disclaimer\u0026ndash; only so many times I can say this). Lots of options out there.\nI hope you may find this exploration useful in your own bulk-uploading needs but, beyond that, that it might serve as a practical introduction to scripting with rclone. rclone is a great tool to have in your back-pocket, and enables working with all kinds of cloud storage providers (not just Google Drive!) in ways that are difficult or impossible using the first-party clients these services provide. You won\u0026rsquo;t need to develop a custom solution utilizing a given cloud storage provider\u0026rsquo;s API\u0026ndash; rclone has already done it for you!\n","permalink":"https://wallacelabs.tech/posts/exploring-google-workspace-upload-limits/","summary":"Google Workspace Enterprise plans (successor to G Suite Business plans) include unlimited Google Drive storage for licensed users, but there is a strict 750Gb limit on data a licensed user can upload to their account within 24 hours. This limit is generally quite reasonable and should not pose an issue. The vast majority of people today do not have internet connections sufficient to upload 750Gb of data within 24 hours even if they wanted to.","title":"Exploring Google Workspace Upload Limits"},{"content":"It seems appropriate that my first post on this site be an exploration of its own technical underpinnings. That being said, before I get too far into the weeds, a bit of context. When I set out to build my site I had a few goals in mind. Those goals can be reasonably summarized as wanting to build a site from markdown files in Github, and self-host it using docker containers. Simple enough, but lets unpack that a bit:\nGoals  Self-Host  Run all website infrastructure in my home, and avoid paying a hosting provider like Squarespace or Wix.   Write all my posts in markdown  Markdown enables easy text-formatting, and allows me to do all my writing in any number of great, markdown-supporting text-editors on basically any computing platform.   Run all infrastructure in containers  Running my web-hosting infrastructure entirely inside docker containers allows me to easily change what computer is running my website in the event that doing so becomes necessary or desirable. Migrating to another computer I own, to a Virtual Private Server (VPS) I rent, or to the Cloud with Azure or AWS container hosting\u0026ndash; it all becomes trivial.   Store the site as a repository in Github  Site files being a Github repo provides me with backups and quick visibility into my site\u0026rsquo;s version history. Additionally, it works alongside the docker containers to keep my site mobile. If I decide to migrate hosts in the future, I won\u0026rsquo;t need to worry about transferring my site files manually. With them in Github, all I would need to do is initiate a git clone of my site\u0026rsquo;s repo on the new host, and the site files are ready to go!    With those goals in mind, I eventually landed on using docker containers of Hugo (to build my site), and NGINX Proxy Manager (to host it). If you would like to do something similar, and build a site like the one you are currently viewing, the below should be helpful! With that out of the way, lets begin.\nRequirements In order to successfully build and host a site using this method, the below are prerequisites:\n Own a domain name. Mine is wallacelabs.tech. If you do not already own one, I recommend purchasing from Hover or Google Domains. As part of owning a domain, you will have control of your domain\u0026rsquo;s DNS records. If you somehow own a domain but do not have the ability to manage its DNS records, you\u0026rsquo;ll have to get that worked out before proceeding. Own an internet-connected computer. This computer will be your web-server. Ideally, that computer will be a server in the sense that it is always on, and dedicated to server functions. While you could host your website on your everyday use computer, I would not recommend it. In terms of actual hardware though? It can be anything from a tiny Raspberry Pi to a massive EPYC Server. The important thing is that it can run docker, is connected to the internet, and is always on. Be able to manage port-forwarding rules on your network\u0026rsquo;s router/gateway. Port-forwarding on your router, along with setting your domain\u0026rsquo;s DNS records, are the two keys required to map traffic from the internet to the web-server hosting your site. Either a Static WAN IP, or a Dynamic DNS configuration. This is required in order for your domain\u0026rsquo;s DNS rules to reliably point traffic to the router on your local network. Most home internet connections do not have static WAN IPs, so you\u0026rsquo;ll likely need to configure a Dynamic DNS (DDNS) service. Stated simply, DDNS is a way of automatically informing your domain\u0026rsquo;s DNS servers of any changes to your home network\u0026rsquo;s Public IP Address. If you use Google Domains as your domain registrar, they have a highly convenient DDNS service built in. If you do not use Google Domains, I would strongly recommend using DuckDNS.   When your website is accessed from the internet, the traffic will follow this general path. Most of the above requirements exist in order to make sure that traffic can reliably traverse this path, allowing your site to be online.\n  If you can check all those boxes, you should be good to go.\nPhase One - Getting Your Site Running Locally Before worrying about domain DNS records, web hosting, and port-forwarding rules, we\u0026rsquo;re going to focus on getting a site running on your local network. First, we\u0026rsquo;ll setup the web-server.\nInstalling Docker In order to setup containers for Hugo and NGINX Proxy Manager, you will need Docker installed on your web-server. Docker runs on Windows, macOS, and Linux, so the Operating System (OS) you use is mostly up to you. It is, however, worth mentioning that less setup is required on Linux and macOS. To run Docker in an ideal manner on Windows, you will need to configure Windows Subsystem for Linux (WSL) and subsequently, a Linux Distribution of your choice to run on WSL. This is not overly complicated, but it is outside the scope of this discussion. A post for another day, perhaps.\n A quick aside: If you plan on using this system as a general purpose home server, in addition to using it as a web-host, it is definitely worth considering the security implications of doing so. By using a computer for web hosting, you are inherently opening up this system to the internet, and there is a lot to be said for the practice of isolating web-hosts on your network through any variety of means. A thorough consideration of these potential risks (and the network hardening measures which might be taken to mitigate them) would be a large digression, and thus is outside the scope of this discussion. Another post for another day, perhaps. For now though, suffice it to say that our risk factor here is not particularly large, due primarily to the fact that we are discussing hosting a static site, not a dynamic one. Had I opted to host a dynamic site using Wordpress, or something of the like, I would be singing a different tune. For additional reading on the security implications of static vs. dynamic sites, see here.\n Begin by installing the Docker Engine using the instructions specific to your OS. Once Docker is installed, you can move on to setting up a Hugo container.\nSetting up a Hugo container To create my site, I utilized a container based on the Hugo Docker Image. Hugo\u0026rsquo;s Docker-Hub image is highly configurable, offering many different tags for specialized integration into custom web-publishing workflows. For my use, all I really wanted was basic container which, when run, provides an interactive shell environment where I can run Hugo commands. The below docker compose file will accomplish this:\n To utilize this compose file to setup your own Hugo container, first, go ahead and download it. Then, open it in your text-editor of choice and update the volume mapping from *PATH TO SITE* in line 6 to wherever you would like your site\u0026rsquo;s git repo to live on your host system. I would recommend creating a folder called site at the root level of your host. If you go that route, just change *PATH TO SITE* to /site and you\u0026rsquo;re good to go. Lastly, open up a Terminal/Powershell session, navigate to whichever folder docker-compose.yml has been downloaded to, and run the following command:\ndocker-compose up You should see the following in your Terminal session:\n Now, open up Docker Desktop and you should see your container running, like this:\n Note that Docker Desktop is only currently available for macOS and Windows, though a Linux version is in Tech Preview, should you wish to try it.\n  With the Hugo container setup and ready to go, you can now use it to create your Hugo site-template.\nCreating your Hugo-Site Now we\u0026rsquo;re in business. Click the \u0026ldquo;CLI\u0026rdquo; button in Docker Desktop to open up a CLI session on your Hugo container. This is how you\u0026rsquo;ll interact with the containerized Hugo program moving forward.\nYour CLI session will have opened up inside the container\u0026rsquo;s /src folder by default. The /src folder is a volume mapped to the host. Whatever data the container puts in this folder, you will be able to access from the host, and vice versa. In docker containers, data inside a volume map like this one is also the only data that will persist after the container is stopped or restarted (i.e. the /src folder is the only thing that does not get deleted when the container stops). The rest of the container is rebuilt from scratch every time the container starts. Given you probably want your Hugo-site to not be deleted after you create it, you will want to create it inside the /src folder. Furthermore, whenever you run Hugo commands, you\u0026rsquo;ll want to run them in this folder. Why? Hugo commands have to be run against a Hugo-site, and this is where your Hugo-site will be located.\nSo, with a CLI session opened at /src, run the below command. When you do, a blank Hugo site template will be initialized as a git repo inside the /src folder. Just be sure to change *YOUR_SITE_NAME* to whatever you would like to call the repo containing your site.\nhugo new site *YOUR_SITE_NAME* The Hugo-site will be generated, and you will see the below message in your CLI session:\n Success!\n  Next up, as indicated in that success message, you\u0026rsquo;ll need to pick a theme and install it. There are tons to choose from! If you are curious, the site you are viewing now is running on PaperMod. Installing your chosen theme is as simple as following the instructions provided by your theme of choice. These instructions usually consist of installing the theme as a git-submodule. This method is convenient as it allows you to easily update your theme by initiating a git pull on that submodule. Put more simply- you\u0026rsquo;ll have a one-click method of updating to a new version of your chosen theme in the future, should you want to do so. As you follow installation instructions, just make sure to run any necessary commands inside the Hugo container\u0026rsquo;s CLI. With a blank Hugo site template created and your theme installed, the foundation of your site is in place.\nCreating your first post With your site created, you can now create a quick test post. Before that though, a brief rabbit-trail about Leaf Bundles.\nTo Leaf-Bundle, or not to Leaf-Bundle  In a moment, you\u0026rsquo;ll run a command that will create the first post on your newly created site. That command is going to create the post as something Hugo calls a Leaf Bundle. I recommend creating all your posts as a leaf bundle because, among other benefits, it allows you to bundle post-images inside the same folder as as the markdown text-file of the post itself. If you do not create your posts as leaf bundles, any images for your posts will be lumped together, completely disorganized, inside a folder named static at the root level of your site. Not ideal. The below diagrams should help visualize this:\n Leaf Bundle Post Structure\n content/ └──posts  └── YOUR_POST_NAME  ├── image1.png  ├── image2.png  └── index.md  Non-Leaf Bundle Post Structure\n content/ ├── posts │ ├── my-post.md │ └── my-other-post.md └── static  ├── my-post_image1.png  ├── my-post_image2.png  ├── my-post_image3.jpg  ├── my-other-post_image1.png  └── my-other-post_image2.jpg  As you accumulate more and more posts, the disorganized pile of images inside /static will continue to get larger and more unwieldy. I highly prefer the structure provided by the leaf bundle method, and I think you will too. Now, back to the business of creating your first post.\nCreating a post (As a Leaf-Bundle!!) Run the below command in the Hugo container CLI to create a test post on your site:\nhugo new content/posts/*YOUR_POST_NAME*/index.md If you wish, feel free to modify index.md to add a quick \u0026ldquo;hello world\u0026rdquo; message, or something of the like, but do not feel obligated. Your first post has been created, whether it has anything written in it or not. With that, time to generate your site!\nGenerating your Site One last command to run now.\nhugo With that, your hugo site has been converted from a pile of markdown documents, images, themes, and the like (that only Hugo can understand) to an actual HTML website that can be displayed by any web browser. You\u0026rsquo;ll find the generated website inside the /public folder created at the root-level of your site\u0026rsquo;s repo.\n This folder is what your web-server will be pushing out to the internet whenever someone accesses your website. Your web-server will only serve the HTML files that are in this folder, so any time you want to update your site, or add a new post, you will need to run the hugo command again to generate your site fresh.\n Hosting your Site! (Locally) Before setting up your newly generated site to be accessed over the internet, we\u0026rsquo;re going to preview it on your local network. Hugo includes a built-in web-host function for this exact purpose. To take advantage of this, run the below command (again, inside the Hugo Container\u0026rsquo;s CLI):\nhugo server With luck, you\u0026rsquo;ll be greeted by the following message:\nOpen http://localhost:1313 in your browser of choice to view your site! As you may have noticed in the above image, the nice thing about previewing your site this way is that Hugo generates the preview in something called \u0026ldquo;Fast Render Mode.\u0026rdquo; Fast Render Mode means you don\u0026rsquo;t have to run the hugo command and regenerate your site every time you want to see a change displayed in your browser. Using the local preview, if you are editing a post, just save any changes you\u0026rsquo;ve made to the index.md file of a given post and you will see them immediately reflected in your browser. Live previewing!\n If you were not so lucky, you may have gotten an error in your CLI session that says something like \u0026ldquo;Check your Hugo installation: you need the extended version to build\u0026hellip;\u0026rdquo;. This is because some themes require an extended version of the Hugo docker image in order to build successfully. Remember earlier, when I said the Hugo image is highly configurable, has many tags, blah blah blah? Here you have a great illustration of that fact. If this happens to you, change line 3 of docker-compose.yml from image: klakegg/hugo:latest to image: klakegg/hugo:ext-alpine. Shut down your container, relaunch it using docker-compose up, and you should be good to go.\n Phase Two - Hosting your Site on the Web! Believe it or not, this should be the easy part. We\u0026rsquo;re going to spin up another container, this time for NGINX Proxy Manager (NPM). NPM (no, not that NPM\u0026hellip; hopefully this is not too confusing) is an awesome web-based GUI built on top of stand-alone NGINX. It is generally used for securely exposing services running on your network to the internet. For example, if you wanted to securely access your Home Assistant Server over the internet, rather than strictly via your home network, or over VPN. In that scenario, you already have a web-server operating a given service or web-app locally, you are just using NPM as a reverse proxy to access that service over the internet. So how can NPM be used to host your site? With a little bit of tricky configuration, NPM can also be used to host static files to the web\u0026ndash; which is exactly what Hugo static sites are!\nBefore setting up NPM, lets quickly configure DNS rules for your domain and add port forwarding rules to your router. Getting this done first prepares the way for traffic to be directed by your domain\u0026rsquo;s nameservers to your web-server running NPM.\nConfiguring your Domain and Router Begin by logging into your domain registrar\u0026rsquo;s website, then pull up the page for managing your domain\u0026rsquo;s DNS records. For me, on Google Domains, it looks like this:\n You will want to create a DNS A record resolving yourDomain.com to your public IP address. Ideally, this address will be static, but most home connections are not. With that in mind, I\u0026rsquo;d recommend configuring Dynamic DNS (DDNS) via Google Domains or Duck DNS, as earlier stated. Without DDNS in place, your site will go down any time the public IP on your home network changes (usually when you reboot your modem). With this DNS record in place, anytime someone accesses yourDomain.com, your domain registrar\u0026rsquo;s nameservers will resolve the request to your home network\u0026rsquo;s router/gateway.\nWith domain DNS rules in place, traffic to your domain will now be pointed to your router. How does your router know what to do with it? Well, you have to tell it what to do. Port-forwarding rules are used to tell your router what traffic to send where. To setup rules telling your router how to handle traffic for your website, you\u0026rsquo;ll begin by logging in to the management interface of your router/gateway. From there, you\u0026rsquo;ll navigate to the Port-Forwarding management page, and add a rule. Unfortunately, I cannot be much more specific than that, as the process will be different on every router. For me though, on a Ubiquiti UniFi based network, it looks like this:\n Make sure to set \u0026lsquo;Forward IP\u0026rsquo; to the IP address of your web-server. Also, double check and make sure your web-server is setup with a static IP address while you\u0026rsquo;re at it. Don\u0026rsquo;t want that changing on you!\n  You\u0026rsquo;re going to need to create two rules here\u0026ndash; one for HTTP traffic coming in over port 80, and another for HTTPS on port 443. You\u0026rsquo;re setting up these rules to point all HTTP and HTTPS traffic coming into your network to your webserver. With those rules in place, you should see something like this:\n You only need to worry about the two rules for HTTP and HTTPS. The other pictured rules are just me censoring my network config.\n  With port forwarding complete, you\u0026rsquo;ve completed the chain such that all traffic going to your domain will reach your web-server, as demonstrated in this image:\n Traffic mapping complete!\n  Setting up an NGINX Proxy Manager container We\u0026rsquo;re nearing the end! Now that\n Your HTML site is sitting in the public folder Your router and domain DNS are configured to point HTTP and HTTPS traffic to your web-server  All thats left is to configure NGINX Proxy Manager to actually serve your website! To do that, use the below docker-compose.yml to create an NPM container on your host. Same drill as before, download the file, and you\u0026rsquo;ll need to make a couple edits before running.\n First, you\u0026rsquo;ll need to chose where on your host system you would like NPM data and LetsEncrypt certificates to live. I recommend creating a folder named container_app_data at the root of your host, with sub-folder NPM. Do that, then just change *PATH* in lines 10 and 11 of docker-compose.yml to /container_app_data/NPM, and docker will automatically create sub-folders for Data and Lets Encrypt at /container_app_data/NPM/data and /container_app_data/NPM/letsencrypt respectively.\nLastly, you\u0026rsquo;ll need to change *PATH TO SITE* in line 12. Set this to be the same as whatever you chose for *PATH TO SITE* when creating the Hugo container. Giving NPM access to *PATH TO SITE*/public will allow it to access your generated site (inside the Hugo container\u0026rsquo;s public folder).\nWith these modifications in place, open a Terminal/Powershell session at the location of docker-compose.yml and run the command:\ndocker-compose up If everything is setup correctly in docker-compose.yml, the container will build and you\u0026rsquo;ll see the following new container in Docker Desktop:\n Two containers now!\n  Configuring NPM for Static-Site Hosting With the NPM container running, open up a browser on your host machine and navigate to http://localhost:81. When you do, you should see the NPM login page:\n To login, use the default username and password:\nEmail: admin@example.com Password: changeme You will then be prompted to setup new login credentials. As with anything, make sure to setup a unique, strong password (I recommend 1Password to help with this). Once you\u0026rsquo;ve setup your new login, click on \u0026ldquo;SSL certificates\u0026rdquo; in the tab bar, then \u0026ldquo;Add SSL Certificate.\u0026rdquo; On this screen, type in your domain name, email and \u0026ldquo;I agree to the Let\u0026rsquo;s Encrypt Terms of Service\u0026rdquo;, like this:\n Before clicking save, make sure to click the \u0026ldquo;Text Server Reachability\u0026rdquo; button. You should see the following message if successful:\n Success here demonstrates traffic from the internet is reaching your web server correctly. Congratulations! You must have setup your domain DNS records and port-forwarding rules successfully. If you don\u0026rsquo;t get a success message here, you might need to go back and double-check those steps. Click the save button, and your SSL certificate will be added:\n With your SSL certificate created, all thats left is to configure a Proxy Host, and your site is live! Click the Hosts tab, then Proxy Hosts. On the Proxy Hosts page, click the Add Proxy Host button. The \u0026ldquo;New Proxy Host\u0026rdquo; screen will pop up. Beginning on the details tab, configure your proxy host like this:\n Obviously be sure to use your actual domain name, not yourDomain.com and www.yourDomain.com\n  Then click on the SSL tab and select the SSL cert you just created, like this:\n Make sure \u0026lsquo;Force SSL\u0026rsquo; and \u0026lsquo;HSTS Enabled\u0026rsquo; are both turned on\n  Lastly, click the Advanced tab, and paste the below as Custom Nginx Configuration.\nlocation / {  root /site/; } like this:\n This is the \u0026rsquo;tricky configuration\u0026rsquo; I mentioned earlier. Thanks to Burke Azbill writing on DimensionQuest.net for figuring this out!\n  This little bit of custom configuration is having NGINX host the static content contained inside the NPM container\u0026rsquo;s /site folder. That folder just so happens to contain your website, as, when you modified the docker-compose.yml for the NPM container, you mapped it to the same location as the public folder your website was generated in. Save this config, and your website should be live! Open up a browser, navigate to your domain name, and there it will be! With your site created, all you have to do now is fill it with content. Congratulations!\nPhase Three - Wrapping Up Lets wrap up by connecting your site up to a Github repo, and discussing some things you can do to enable some general quality-of-life improvements as you manage and continue to develop your site.\nConnecting to GitHub When you first used Hugo to create your site, it initialized your site template inside a git repository. All you need to do now is add this existing repository to GitHub. There are many ways to accomplish this, and this tutorial has gone on long enough, so I won\u0026rsquo;t be verbosely outlining a specific one. In general though, you just need a git client that integrates with Github. Github Desktop works great on Windows and macOS, and there are unofficial forks that run on Linux. If you hate yourself, you can do it exclusively using git in the command-line (I\u0026rsquo;m mostly joking- but if you\u0026rsquo;re working with git exclusively via CLI, you probably don\u0026rsquo;t need to be reading this). You could also do it with tools like Tower, or GitKraken, both premium GUI git clients requiring for-pay licenses.\nPersonally, I enjoy using the source-control integrations built directly into text editors like Atom or Visual Studio Code. With these tools, I can write posts, modify site configuration files, and push up to Github all in one program. Makes it all quite easy.\nUltimately, all you need to do is add your existing local git repo to Github. Pick your favorite way, or try a couple!\nQuality of Life This may sound counterintuitive, but I would recommend setting up an additional development environment on your everyday desktop and/or laptop if you have them. Working directly on the web-server is likely a bit incontinent, if you\u0026rsquo;ve truly set it up to act as a server. Most of the time, my server runs completely headless, with no monitor, keyboard, or mouse attached, but it is running my site\u0026rsquo;s docker containers all the same. Day to day, I do most of my site\u0026rsquo;s development from my laptop. How does that work? This is the benefit of having a site running on docker containers and a Github repository.\nOn my laptop, I\u0026rsquo;ve installed Visual Studio Code, and used it to clone my site\u0026rsquo;s GitHub repo locally. From there, I\u0026rsquo;m able to edit my site as I please. I also have Docker Desktop installed on my laptop, running Hugo inside a container, as we discussed earlier. If I want to preview any changes I\u0026rsquo;ve made, I just start up the Hugo server inside the container, and preview my changes from http://localhost:1313. I usually have the local Hugo server running as I write so that any time I want to check my work, I can quickly save the post I\u0026rsquo;m writing, switch windows, double check how the site will look, and switch back to resume my writing. Whenever I am done with a post and ready to publish, I save my work and run the hugo command inside my local docker container to generate my site fresh inside the public folder. From there, I initiate a git push to sync my changes to GitHub, and remotely initiate a git pull on my web server to sync the changes back down from Github. Boom, changes are live on the web!\nYou can do this on as many computers as you like, enabling truly mobile site development. If you can live without running Hugo locally for previewing, you can even publish to your site from an iPad or smartphone. All you need is a git client. If you want to get really fancy, you can even add another docker container to your web-server called Code-Server. This container enables you to run Visual Studio Code inside a docker container, and interact with it through a web-page. Then you effectively have a full development environment running inside docker containers on your web-server, which can all be remotely accessed over your network.\nLong story short, you have no shortage of options when it comes to convenient ways to manage your site. Figure out which setup is most beneficial to your use-case, and you can make it happen pretty easily.\nWith that, our tale has been told. If you followed along, you should now have a site live on the internet, and many tools at your disposal to manage it. As you begin to create content for your site, you may find it helpful to see additional commands you can run with Hugo, what type of markdown is supported by Hugo as you write posts, and shortcodes Hugo supports for special formatting on top of markdown. Lastly, don\u0026rsquo;t forget to read your theme\u0026rsquo;s documentation for special functionality it may provide to your site (many themes have custom shortcodes for adding additional formatting possibilities to your site, beyond basic markdown and Hugo\u0026rsquo;s built in shortcodes). Thank you for reading, and good luck to you, as the real work has only just begun- time to get writing!\n","permalink":"https://wallacelabs.tech/posts/static-site-hosting-with-docker-hugo-and-nginx-proxy-manger/","summary":"It seems appropriate that my first post on this site be an exploration of its own technical underpinnings. That being said, before I get too far into the weeds, a bit of context. When I set out to build my site I had a few goals in mind. Those goals can be reasonably summarized as wanting to build a site from markdown files in Github, and self-host it using docker containers.","title":"Static Site Hosting for Free with Docker, Hugo, and NGINX Proxy Manger"}]